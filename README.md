# Project for Gesture Based UI Development

## Brief
Develop an application with a Natural User Interface. You have a choice of technologies available to
you and an opportunity to combine a lot of technology that you have worked with over the past four
years.

At the very least, this should be a local implementation of the application using gestures to interact
with it. You can expand out to include real-world hardware and use this as an opportunity to prove
a concept. The Internet of Things is a common phrase, so you could implement a solution taking
advantage of hardware like the Raspberry Pi, using the cloud for data transfer and creating a realworld
scenario through this medium. 

### Purpose of Application 
The purpose of my application was to develop a game, thats controlled using my body. It is a simple game, inspired by space invaders. Where the object is moved by leaning body in the direction you want it to move, either left or right. You can fire by pointed your fingers. There are 3 levels easy, normal, hard that are loaded as you play the game. After reaching a certain score you progress to next scene/level which is more challenging.

### Gesture identified as appropriate for this application
The gestures implemented into the game are simple controls. The main object is moved left or right by either leaning left or right. The object has a shooting action and can be activated by pointing fingers up. The menu can also be interacted with using these features.

### Hardware used in creating the application
For this gesture based UI project I used the Kinect 2.0, an updated natural user interface sensor. It was redesigned and recreated to provide motion-tracking and voice commands for the Xbox One. Is equipped with 1080p color camera that enables multiple features such as high-quality Skype video chats, act as a 3D scanning device that is strong enough to discern buttons and folds on a shirt, as well as whether you're moving your fingers, if you're facing the sensor or not, and even your facial expression. It will also track up to six people (or "skeletons,"). It enables a developer to add a lot of functionality to a project.

### Architecture for the solution
This project was built using [Unity](https://store.unity.com/) 2017.4.0f1 cross platform engine and [KinectForWindows_UnityPro](https://developer.microsoft.com/en-us/windows/kinect) 2.0.1410. 

### Conclusions & Recommendations
This was my first project built using Unity and it took a while to become familiar with its features. The project a was quite challenging as I was unfamiliar developing using gesture inputs as opposed to keyboard inputs. If I was to further develop this project I would like utilize more body functions, and hand gestures.
## References
* [Kinect for Windows SDK 2.0](https://developer.microsoft.com/en-us/windows/kinect)
* [Unity](https://store.unity.com/)